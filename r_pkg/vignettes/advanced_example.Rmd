---
title: "Building a Random Forest Predictive Model with the SciDB Streaming Interface and R"
date: "`r Sys.Date()`"
author: B. W. Lewis, Copyright (C) 2016 by Paradigm4, Inc.
output:
  html_document:
    theme: flatly
    highlight: tango
    mathjax: null
vignette: >
  \VignetteIndexEntry{scidbstrm_predict}
  \VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---

# Topics

- SciDB streaming API and R
- Predictive modeling
- Parallel computing
- Storing generic R model output in SciDB

## Prerequisites

You need SciDB and R of course, and the following add ons for each:

### SciDB
- `accelerated_io_tools`
- `grouped_aggregate`
- `superfunpack`
- `stream`

### R
- `randomForests`
- `base64enc`
- `scidbstrm`

The scidbstrm package for R is available from https://github.com/paradigm4/streaming/r_pkg. You can
install it with:
```
sudo R --slave -e  "devtools::install_github('paradigm4/streaming', subdir='r_pkg')"
```
Be sure to install all packages so that they are available to all R users!

# Introduction

This vignette walks through, from start to finish, building a supervised
predictive model using R and SciDB. Model training is conducted in
parallel and managed through the new SciDB streaming API.


We use a wearables data set from
http://groupware.les.inf.puc-rio.br/har#dataset.  The data consist of
descriptive statistics and processed data from three accelerometer device
measurements attached to various people performing five prescribed activities
coded as "class" and look like:
```
gender age how_tall_in_meters weight body_mass_index x1  y1   z1   x2   y2   z2  x3  y3   z3   x4   y4   z4       class
Woman  46               1.62     75            28.6  -3  95  -70   11   16  -61  28  76  -83 -187  -91 -153  standingup
Woman  46               1.62     75            28.6 -10  94  -98    0   74 -121  15 103  -89 -162 -104 -157    standing
Woman  28               1.58     55            22.0  11 106 -116 -493 -516 -616  12 128 -119 -176  -82 -183     walking
  Man  31               1.71     83            28.4   2  66  -64  -20    6  -27  66  59 -104  -97 -123 -170     sitting
  Man  31               1.71     83            28.4 -43  49 -132  -17   97 -135 -47 168 -103 -210 -100 -153     walking
Woman  28               1.58     55            22.0 -11 104 -120 -493 -516 -616  38  34 -114 -187  -73 -159     walking
  Man  75               1.67     67            24.0 -11  97 -135   -2  -39  -23 -11 119  -94 -195 -115 -102 sittingdown
```
Our objective is, given these data, develop a model that predicts "class." This
example is representative of many supervised classification
problems.

We model the data using a variation of the Random Forest algorithm developed
by Leo Breiman and Adele Cutler, see for example
https://www.stat.berkeley.edu/~breiman/randomforest2001.pdf
and http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm
and the references therein, and the R package implementation by Andy
Liaw (https://cran.r-project.org/web/packages/randomForest).

We modify the default Random Forest algorithm implementation to enhance
parallel scalability for large data set sizes using the Big Data Bootstrap
approach  of Kleiner et. al.  (A. Kleiner, A. Talwalkar, P. Sarkar, and M.
Jordan, “A scalable bootstrap for massive data,” Journal of the Royal
Statistical Society: Series B, vol. 76, no. 4, pp.  795–816, 2014)
following a recently proposed implementation by Genuer, Poggi,
Tuleau-Malot and Villa-Vialaneix (http://arxiv.org/pdf/1511.08327v1.pdf).
Parallelization of the Random Forest model training is managed using the SciDB
streaming API and R.

## Obtaining and loading data

Download and uncompress the data with:
```
cd /tmp
wget http://groupware.les.inf.puc-rio.br/static/har/dataset-har-PUC-Rio-ugulino.zip
unzip dataset-har-PUC-Rio-ugulino.zip
```
producing a file named `dataset-har-PUC-Rio-ugulino.csv`. Despite its name, the
input file is delimited by the semicolon ";" character.

The data arrive requiring some very minor cleanup including filtering out one
bad value in the z4 variable and removing trailing carriage return characters
in the class variable. Data are loaded into a 165,632 row SciDB array named HAR.
```
iquery -naq "
store(
filter(
project(
unpack(
apply(
  aio_input('/tmp/dataset-har-PUC-Rio-ugulino.csv', 'num_attributes=19', 'attribute_delimiter=;', 'header=1'),
    gender, dcast(rsub(rsub(a1, 's/W.*/0/'), 's/M.*/1/'), int32(null)),
    age, dcast(a2, double(null)),
    height, dcast(rsub(a3, 's/,/\./'), double(null)),
    weight, dcast(a4, double(null)),
    bmi, dcast(rsub(a5, 's/,/\./'), double(null)),
    x1, dcast(a6, int32(null)),
    y1, dcast(a7, int32(null)),
    z1, dcast(a8, int32(null)),
    x2, dcast(a9, int32(null)),
    y2, dcast(a10, int32(null)),
    z2, dcast(a11, int32(null)),
    x3, dcast(a12, int32(null)),
    y3, dcast(a13, int32(null)),
    z3, dcast(a14, int32(null)),
    x4, dcast(a15, int32(null)),
    y4, dcast(a16, int32(null)),
    z4, dcast(a17, int32(null)),
    class, rsub(a18, 's/\r//')),
  i),
  gender, age, height, weight, bmi,
  x1, y1, z1,
  x2, y2, z2,
  x3, y3, z3,
  x4, y4, z4, class),
not is_null(z4)),
HAR)"
``` 

The five possible classes are shown below. Importantly, we will manually code
these as contrast-encoded factors for our Random Forest model. (Alternatively,
and more efficiently, we could have just encoded these as integers in SciDB.)
```
iquery -aq "grouped_aggregate(HAR, class, max(class))"

{instance_id,value_no} class,class_max
{0,0} 'sitting','sitting'
{2,0} 'sittingdown','sittingdown'
{4,0} 'standingup','standingup'
{4,1} 'walking','walking'
{6,0} 'standing','standing'
```

The following SciDB query partitions the data into 8 SciDB chunks, randomizing
all the data across the chunks, and then splits the data roughly into two
halves. We plan to use the 1st half of the data as a training set on which to
build a model, and then we'll test the model on the remainder.
```
vars="gender, age, height, weight, bmi, x1, y1, z1, x2, y2, z2, x3, y3, z3, x4, y4, z4, class"
iquery -naq "store(apply(redimension(sort(apply(HAR, p, random() % 8), p), <gender:int32,age:double,height:double,weight:double,bmi:double,x1:int32,y1:int32,z1:int32,x2:int32,y2:int32,z2:int32,x3:int32,y3:int32,z3:int32,x4:int32,y4:int32,z4:int32,class:string> [n=0:*,20704,0]), train, bool(n % 2)), HART)"
TRAIN="project(filter(HART, train), $vars)"  # Training data
TEST="project(filter(HART, not train), $vars)"  # Test data
```
We choose to split the problem up into 8 chunks because we're running on an
8-instance SciDB cluster. For larger data we could, for example, split the
problem up into any number of chunks that meet a desired sub-problem size
(this approach should scale well for large data).

Note the sizes of the split data sets for future reference below:
```
iquery -aq "op_count($TRAIN)"
{i} count
{0} 82816

iquery -aq "op_count($TEST)"
{i} count
{0} 82816
```

<!--
```
cat << END | iquery -a -f /dev/stdin
stream($P, 'R --slave -e "library(scidbstrm); map(function(x) data.frame(n=as.character(nrow(x)), stringsAsFactors=FALSE))"',
       'format=df', 'types=string')
END
```
-->

# Training a model

The following R program computes the *little bootstrap* phase of the big data
bootstrap process. It runs in parallel on each of the 8 data partitions defined
above. The output of the program is a set of 8 serialized R randomForest
model objects. We'll combine those into a single model later. Save this
program in the file `/tmp/bootstrap.R`.

```{r}
library(scidbstrm)
library(base64enc)
library(randomForest)

#' little bootstrap function
#' Combine resample data {x, y} rep times and combine randomForest(x,y,ntree) models
#' @param x y Random Forest input data and training vector
#' @param N full data number of rows
#' @param ntree number of Random Forest trees
#' @param rep number of bootstrap repetitions
lb <- function(x, y, N, ntree, rep)
{ 
  n <- nrow(x)
  Reduce(combine, Map(function(dummy) {
    i <- sample(n, n, replace=TRUE)
    r <- randomForest(x[i, ], y[i], ntree=ntree, norm.votes=FALSE)
    # Sum replicated rows from the bootstrap and then reassign in order
    # (Note: can speed up this step with, for example, data.table!)
    d <- data.frame(r$votes, row.names=c())
    d <- aggregate(d, by=list(row=as.integer(rownames(r$votes))), FUN=sum)
    # Omit oob.times vector (cheap, we could order it and fill in like the vote matrix instead)
    r$oob.times <- 0
    i <- which((names(d) %in% colnames(r$votes)))
    # Assign summed votes back in to the model object 'votes' matrix
    V <- matrix(0, nrow=N, ncol=ncol(r$votes))
    V[d$row, ] = as.matrix(d[, i])
    r$votes <- V
    r
  }, 1:rep))
}

# Our SciDB streaming function explicitly applies consistent factor levels
# to the "class" variable, presented to us as character values from SciDB.
# We return the little bootstrapped Random Forest model as a serialized
# R object, encoded in a character string since the SciDB streaming API
# does not yet support binary blobs.

levels <- c("sitting", "sittingdown", "standing", "standingup", "walking")

f <- function(x)
{
  x$class <- factor(x$class, levels=levels)
  mdl <- lb(x[, -18], x[[18]], 82816, 10, 2)
  data.frame(x=base64encode(serialize(mdl, NULL)), stringsAsFactors=FALSE)
}

map(f)
```

## Combining models

The following simple R program combines the results of many Random Forest
models into a single model. Save the program as the file `/tmp/combine.R`.

```{r}
# Combine a bunch of Random Forest models into a single model

library(scidbstrm)
library(base64enc)
library(randomForest)

result <- NULL

f <- function(x)
{
  model <- unserialize(base64decode(x[,1]))
  if(is.null(result)) result <<- model 
  else result <<- combine(result, model)
  NULL
}

final <- function(x)
{
  data.frame(x=base64encode(serialize(result, NULL)), stringsAsFactors=FALSE)
}

map(f, final=final)
```

## Running everything in parallel from SciDB

The following command runs the little bootstrap code in parallel on the
training data and combines the results in a single query, storing the output to
a SciDB array named "model".

```
iquery -naq "store(stream(stream($TRAIN, 'Rscript /tmp/bootstrap.R', 'format=df', 'types=string'),
                     'Rscript /tmp/combine.R', 'format=df', 'types=string'), model)"
```


# Model Prediction

Now that we've trained and stored a model in SciDB let's run the test data
through it and make predictions. The following simple R program does that,
returning two columns of observed (true) values and our model-predicted
values for comparison:

```{r}
library(scidbstrm)
library(base64enc)
library(randomForest)

# Obtain the model from SciDB
model <- unserialize(base64decode(getChunk()[[1]]))

# Predict and return two columns: observed (true) data and our prediction
f <- function(x)
{
  p <- predict(model, newdata=x)
  data.frame(observed=x[[18]],  predicted=as.character(p), stringsAsFactors=FALSE)
}

# Stream SciDB data through our model prediction function
map(f)
```

Save that program to the file `/tmp/predict.R` for use in the next query.  We
use an advanced technique in the streaming API for supplying more than one
input to our child processes together with the low-level `getChunk()` function
in the scidbstrm R package. That obtains the model first, and then streams data
through it.

The following query runs the held-out test data through our model, saving the
results into a SciDB array named "predictions":
```
iquery -aq "store(stream($TEST, 'Rscript /tmp/predict.R', 'format=df', 'types=string,string', _sg(model,0,0)), predictions)"
```

We can get a better feel for how well our model works by making a cross
tabulation of the results in R (interactive R code follows), sometimes
called a confusion matrix.
```{r}
library(scidb)
scidbconnect()
p = scidb("predictions")[]
p = p[,4:5]
names(p) = c("observed", "predicted")
table(p)
```
```
             predicted
observed      sitting sittingdown standing standingup walking
  sitting       25273           6        0         34       2
  sittingdown      44        5537       63        181      89
  standing          0           3    23494         23     166
  standingup       47         189      148       5704     119
  walking           0          30      144         33   21487
```
Our model predictions closely track the observations, not bad!
